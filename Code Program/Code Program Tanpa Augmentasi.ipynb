{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bad096",
   "metadata": {},
   "source": [
    "# 1. Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb899bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f17ce",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f9c17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset.h5 (tanpa augmentasi)\n"
     ]
    }
   ],
   "source": [
    "# Load data dari dataset.h5\n",
    "with h5py.File(\"dataset.h5\", \"r\") as hf:\n",
    "    x_train = hf[\"train_images\"][:]\n",
    "    y_train = hf[\"train_labels\"][:]\n",
    "    x_val = hf[\"val_images\"][:]\n",
    "    y_val = hf[\"val_labels\"][:]\n",
    "\n",
    "print(\"Loaded dataset.h5 (tanpa augmentasi)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ad8a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kelas dari y_train.shape[1]: 8\n"
     ]
    }
   ],
   "source": [
    "# Cek len(label_map) ==  y_train.shape[1]\n",
    "# print(\"Jumlah kelas dari label_map      :\", len(label_map))\n",
    "print(\"Jumlah kelas dari y_train.shape[1]:\", y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d3501",
   "metadata": {},
   "source": [
    "# 3. Build and Train 3 Model CNN-ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bd5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model function\n",
    "def build_resnet_model(resnet_version, num_classes, learning_rate):\n",
    "    base_model = resnet_version(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training config\n",
    "resnet_versions = [ResNet50, ResNet101, ResNet152]\n",
    "batch_sizes = [16, 32] # 64, 128\n",
    "learning_rate = 0.01\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1us/step\n",
      "Training ResNet50 WITHOUT AUGMENTATION, batch_size=16\n",
      "Epoch 1/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1864s\u001b[0m 9s/step - accuracy: 0.1782 - loss: 3.3242 - val_accuracy: 0.1294 - val_loss: 2.4769\n",
      "Epoch 2/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1597s\u001b[0m 8s/step - accuracy: 0.3094 - loss: 1.7700 - val_accuracy: 0.1327 - val_loss: 2.6675\n",
      "Epoch 3/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1337s\u001b[0m 7s/step - accuracy: 0.3677 - loss: 1.5891 - val_accuracy: 0.1316 - val_loss: 2.9079\n",
      "Epoch 4/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1108s\u001b[0m 6s/step - accuracy: 0.4266 - loss: 1.4396 - val_accuracy: 0.2400 - val_loss: 2.3658\n",
      "Epoch 5/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1082s\u001b[0m 5s/step - accuracy: 0.4569 - loss: 1.3362 - val_accuracy: 0.1814 - val_loss: 11.0585\n",
      "Epoch 6/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1128s\u001b[0m 6s/step - accuracy: 0.4891 - loss: 1.2813 - val_accuracy: 0.2058 - val_loss: 4.7386\n",
      "Epoch 7/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 6s/step - accuracy: 0.5190 - loss: 1.2113 - val_accuracy: 0.2190 - val_loss: 4.6596\n",
      "Epoch 8/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1158s\u001b[0m 6s/step - accuracy: 0.5508 - loss: 1.1760 - val_accuracy: 0.3850 - val_loss: 1.7839\n",
      "Epoch 9/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1119s\u001b[0m 6s/step - accuracy: 0.5851 - loss: 1.0570 - val_accuracy: 0.4491 - val_loss: 1.6520\n",
      "Epoch 10/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1087s\u001b[0m 5s/step - accuracy: 0.6499 - loss: 0.9102 - val_accuracy: 0.1504 - val_loss: 7.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m171446536/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 1us/step\n",
      "Training ResNet101 WITHOUT AUGMENTATION, batch_size=16\n",
      "Epoch 1/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2291s\u001b[0m 11s/step - accuracy: 0.1606 - loss: 3.4419 - val_accuracy: 0.1327 - val_loss: 2.1376\n",
      "Epoch 2/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1970s\u001b[0m 10s/step - accuracy: 0.2462 - loss: 1.8738 - val_accuracy: 0.1327 - val_loss: 2.3913\n",
      "Epoch 3/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2013s\u001b[0m 10s/step - accuracy: 0.3105 - loss: 1.7362 - val_accuracy: 0.1327 - val_loss: 2.5885\n",
      "Epoch 4/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1924s\u001b[0m 10s/step - accuracy: 0.3506 - loss: 1.6434 - val_accuracy: 0.2434 - val_loss: 4.4663\n",
      "Epoch 5/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1973s\u001b[0m 10s/step - accuracy: 0.3834 - loss: 1.5515 - val_accuracy: 0.2898 - val_loss: 1.8297\n",
      "Epoch 6/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2019s\u001b[0m 10s/step - accuracy: 0.3916 - loss: 1.5195 - val_accuracy: 0.3075 - val_loss: 1.9930\n",
      "Epoch 7/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1911s\u001b[0m 10s/step - accuracy: 0.4443 - loss: 1.4330 - val_accuracy: 0.2533 - val_loss: 3.4629\n",
      "Epoch 8/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2095s\u001b[0m 11s/step - accuracy: 0.4932 - loss: 1.3145 - val_accuracy: 0.2279 - val_loss: 3.2054\n",
      "Epoch 9/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2591s\u001b[0m 13s/step - accuracy: 0.5184 - loss: 1.2514 - val_accuracy: 0.2279 - val_loss: 5.4129\n",
      "Epoch 10/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2208s\u001b[0m 11s/step - accuracy: 0.5341 - loss: 1.1964 - val_accuracy: 0.1836 - val_loss: 3.5432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m234698864/234698864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 0us/step\n",
      "Training ResNet152 WITHOUT AUGMENTATION, batch_size=16\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Tanpa Augmentasi\n",
    "for batch_size in batch_sizes:\n",
    "    for resnet in resnet_versions:\n",
    "        model = build_resnet_model(resnet, num_classes, learning_rate)\n",
    "        print(f\"Training {resnet.__name__} WITHOUT AUGMENTATION, batch_size={batch_size}\")\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=10\n",
    "        )\n",
    "        filename = f\"{resnet.__name__}_noaug_bs{batch_size}.h5\"\n",
    "        model.save(filename)\n",
    "        models[filename] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa6b57b-046c-4afc-8acc-2bcba7d4f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ResNet50_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet101_noaug_bs16.h5, already exists.\n",
      "Training ResNet152 WITHOUT AUGMENTATION, batch_size=16\n",
      "Epoch 1/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3081s\u001b[0m 15s/step - accuracy: 0.1375 - loss: 3.4878 - val_accuracy: 0.1316 - val_loss: 2.0774\n",
      "Epoch 2/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2607s\u001b[0m 13s/step - accuracy: 0.1524 - loss: 2.0748 - val_accuracy: 0.1350 - val_loss: 2.0793\n",
      "Epoch 3/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2513s\u001b[0m 13s/step - accuracy: 0.1299 - loss: 2.0811 - val_accuracy: 0.1173 - val_loss: 2.0837\n",
      "Epoch 4/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2644s\u001b[0m 13s/step - accuracy: 0.1575 - loss: 2.0760 - val_accuracy: 0.1527 - val_loss: 2.0594\n",
      "Epoch 5/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2606s\u001b[0m 13s/step - accuracy: 0.1479 - loss: 2.0681 - val_accuracy: 0.1648 - val_loss: 2.0572\n",
      "Epoch 6/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2700s\u001b[0m 14s/step - accuracy: 0.1662 - loss: 2.0623 - val_accuracy: 0.1692 - val_loss: 2.0440\n",
      "Epoch 7/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2617s\u001b[0m 13s/step - accuracy: 0.1819 - loss: 2.0477 - val_accuracy: 0.1670 - val_loss: 2.0611\n",
      "Epoch 8/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2542s\u001b[0m 13s/step - accuracy: 0.1824 - loss: 2.0397 - val_accuracy: 0.1460 - val_loss: 2.0934\n",
      "Epoch 9/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2525s\u001b[0m 13s/step - accuracy: 0.1879 - loss: 2.0547 - val_accuracy: 0.1338 - val_loss: 2.1045\n",
      "Epoch 10/10\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2698s\u001b[0m 14s/step - accuracy: 0.1826 - loss: 2.0479 - val_accuracy: 0.1582 - val_loss: 2.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50 WITHOUT AUGMENTATION, batch_size=32\n",
      "Epoch 1/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.1554 - loss: 5.4011 "
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 519. MiB for an array with shape (904, 224, 224, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m model = build_resnet_model(resnet, num_classes, learning_rate)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresnet.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m WITHOUT AUGMENTATION, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m model.save(filename)\n\u001b[32m     20\u001b[39m models[filename] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[33;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np.ndarray):\n\u001b[32m     93\u001b[39m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[32m     94\u001b[39m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[32m     95\u001b[39m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m   value = value.copy()\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops.EagerTensor):\n\u001b[32m     98\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value.dtype != dtype:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 519. MiB for an array with shape (904, 224, 224, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Tanpa Augmentasi\n",
    "for batch_size in batch_sizes:\n",
    "    for resnet in resnet_versions:\n",
    "        filename = f\"{resnet.__name__}_noaug_bs{batch_size}.h5\"\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Skipping {filename}, already exists.\")\n",
    "            continue  # Lewati model ini karena sudah dilatih dan disimpan\n",
    "\n",
    "        model = build_resnet_model(resnet, num_classes, learning_rate)\n",
    "        print(f\"Training {resnet.__name__} WITHOUT AUGMENTATION, batch_size={batch_size}\")\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=10\n",
    "        )\n",
    "        model.save(filename)\n",
    "        models[filename] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249538c-2dcb-4152-9a6b-ecbd457fd36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ResNet50_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet101_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet152_noaug_bs16.h5, already exists.\n",
      "Training ResNet50 WITHOUT AUGMENTATION, batch_size=32\n",
      "Epoch 1/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1399s\u001b[0m 14s/step - accuracy: 0.1390 - loss: 4.7336 - val_accuracy: 0.1128 - val_loss: 2214.0581\n",
      "Epoch 2/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1138s\u001b[0m 11s/step - accuracy: 0.1802 - loss: 2.0210 - val_accuracy: 0.1338 - val_loss: 2.0803\n",
      "Epoch 3/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1178s\u001b[0m 12s/step - accuracy: 0.1678 - loss: 2.0252 - val_accuracy: 0.1162 - val_loss: 2.1379\n",
      "Epoch 4/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 11s/step - accuracy: 0.2120 - loss: 1.9826 - val_accuracy: 0.1162 - val_loss: 2.1959\n",
      "Epoch 5/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1120s\u001b[0m 11s/step - accuracy: 0.2190 - loss: 1.9545 - val_accuracy: 0.1449 - val_loss: 2.1636\n",
      "Epoch 6/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 11s/step - accuracy: 0.2388 - loss: 1.9390 - val_accuracy: 0.1327 - val_loss: 2.2986\n",
      "Epoch 7/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1139s\u001b[0m 12s/step - accuracy: 0.2682 - loss: 1.8532 - val_accuracy: 0.1504 - val_loss: 5.3906\n",
      "Epoch 8/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1116s\u001b[0m 11s/step - accuracy: 0.3348 - loss: 1.6928 - val_accuracy: 0.2223 - val_loss: 3.8440\n",
      "Epoch 9/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1147s\u001b[0m 12s/step - accuracy: 0.4123 - loss: 1.4933 - val_accuracy: 0.2588 - val_loss: 3.3598\n",
      "Epoch 10/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1141s\u001b[0m 12s/step - accuracy: 0.4786 - loss: 1.3540 - val_accuracy: 0.2865 - val_loss: 4.0969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet101 WITHOUT AUGMENTATION, batch_size=32\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Tambahkan fungsi ini:\n",
    "def create_tf_dataset(x, y, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(x))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Training loop\n",
    "for batch_size in batch_sizes:\n",
    "    for resnet in resnet_versions:\n",
    "        filename = f\"{resnet.__name__}_noaug_bs{batch_size}.h5\"\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Skipping {filename}, already exists.\")\n",
    "            continue\n",
    "\n",
    "        model = build_resnet_model(resnet, num_classes, learning_rate)\n",
    "        print(f\"Training {resnet.__name__} WITHOUT AUGMENTATION, batch_size={batch_size}\")\n",
    "\n",
    "        train_dataset = create_tf_dataset(x_train, y_train, batch_size)\n",
    "        val_dataset = create_tf_dataset(x_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=10\n",
    "        )\n",
    "\n",
    "        model.save(filename)\n",
    "        models[filename] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a25fe7-8396-417d-83a9-a250e2f68472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ResNet50_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet101_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet152_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet50_noaug_bs32.h5, already exists.\n",
      "Training ResNet101 WITHOUT AUGMENTATION, batch_size=32\n",
      "Epoch 1/10\n",
      "\u001b[1m 1/99\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:50:24\u001b[0m 178s/step - accuracy: 0.0938 - loss: 2.1724"
     ]
    }
   ],
   "source": [
    "# Tambahkan fungsi ini:\n",
    "def create_tf_dataset(x, y, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(x))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Training loop\n",
    "for batch_size in batch_sizes:\n",
    "    for resnet in resnet_versions:\n",
    "        filename = f\"{resnet.__name__}_noaug_bs{batch_size}.h5\"\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Skipping {filename}, already exists.\")\n",
    "            continue\n",
    "\n",
    "        model = build_resnet_model(resnet, num_classes, learning_rate)\n",
    "        print(f\"Training {resnet.__name__} WITHOUT AUGMENTATION, batch_size={batch_size}\")\n",
    "\n",
    "        train_dataset = create_tf_dataset(x_train, y_train, batch_size)\n",
    "        val_dataset = create_tf_dataset(x_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=10\n",
    "        )\n",
    "\n",
    "        model.save(filename)\n",
    "        models[filename] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a4e312-dd5a-48ab-b42c-0b6f6401495e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ResNet50_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet101_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet152_noaug_bs16.h5, already exists.\n",
      "Skipping ResNet50_noaug_bs32.h5, already exists.\n",
      "Training ResNet101 WITHOUT AUGMENTATION, batch_size=32\n",
      "Epoch 1/10\n",
      "99/99 [==============================] - 2308s 22s/step - loss: 2.5517 - accuracy: 0.1435 - val_loss: 2.0793 - val_accuracy: 0.1350\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 2150s 22s/step - loss: 2.0868 - accuracy: 0.1305 - val_loss: 2.0780 - val_accuracy: 0.1350\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 2094s 21s/step - loss: 2.0793 - accuracy: 0.1308 - val_loss: 2.0780 - val_accuracy: 0.1350\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 2100s 21s/step - loss: 2.0801 - accuracy: 0.1315 - val_loss: 2.0867 - val_accuracy: 0.1338\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 2109s 21s/step - loss: 2.0786 - accuracy: 0.1318 - val_loss: 2.0793 - val_accuracy: 0.1294\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 2111s 21s/step - loss: 2.0787 - accuracy: 0.1210 - val_loss: 2.1029 - val_accuracy: 0.1040\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 2108s 21s/step - loss: 2.0756 - accuracy: 0.1444 - val_loss: 2.0789 - val_accuracy: 0.1338\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 3862s 39s/step - loss: 2.0721 - accuracy: 0.1375 - val_loss: 2.0797 - val_accuracy: 0.1350\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 3468s 35s/step - loss: 2.0765 - accuracy: 0.1435 - val_loss: 2.0678 - val_accuracy: 0.1560\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 2174s 22s/step - loss: 2.0751 - accuracy: 0.1422 - val_loss: 2.1524 - val_accuracy: 0.1405\n",
      "Training ResNet152 WITHOUT AUGMENTATION, batch_size=32\n",
      "Epoch 1/10\n",
      "99/99 [==============================] - 3187s 31s/step - loss: 2.7573 - accuracy: 0.1498 - val_loss: 19.7541 - val_accuracy: 0.1062\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 3083s 31s/step - loss: 1.9435 - accuracy: 0.1944 - val_loss: 2.2249 - val_accuracy: 0.1327\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 3774s 38s/step - loss: 1.8525 - accuracy: 0.2418 - val_loss: 2.2481 - val_accuracy: 0.1294\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 4932s 50s/step - loss: 1.7977 - accuracy: 0.2620 - val_loss: 2.3147 - val_accuracy: 0.1327\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 3082s 31s/step - loss: 1.7853 - accuracy: 0.2661 - val_loss: 2.3384 - val_accuracy: 0.1294\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 3066s 31s/step - loss: 1.7708 - accuracy: 0.2750 - val_loss: 2.4458 - val_accuracy: 0.1327\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 3065s 31s/step - loss: 1.7227 - accuracy: 0.2971 - val_loss: 2.6141 - val_accuracy: 0.1327\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 3097s 31s/step - loss: 1.6760 - accuracy: 0.3211 - val_loss: 2.1286 - val_accuracy: 0.2058\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 3083s 31s/step - loss: 1.6027 - accuracy: 0.3328 - val_loss: 2.1294 - val_accuracy: 0.2157\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 3076s 31s/step - loss: 1.5257 - accuracy: 0.3638 - val_loss: 1.8886 - val_accuracy: 0.2566\n"
     ]
    }
   ],
   "source": [
    "# Tambahkan fungsi ini:\n",
    "def create_tf_dataset(x, y, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(x))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Training loop\n",
    "for batch_size in batch_sizes:\n",
    "    for resnet in resnet_versions:\n",
    "        filename = f\"{resnet.__name__}_noaug_bs{batch_size}.h5\"\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Skipping {filename}, already exists.\")\n",
    "            continue\n",
    "\n",
    "        model = build_resnet_model(resnet, num_classes, learning_rate)\n",
    "        print(f\"Training {resnet.__name__} WITHOUT AUGMENTATION, batch_size={batch_size}\")\n",
    "\n",
    "        train_dataset = create_tf_dataset(x_train, y_train, batch_size)\n",
    "        val_dataset = create_tf_dataset(x_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=10\n",
    "        )\n",
    "\n",
    "        model.save(filename)\n",
    "        models[filename] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb848919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
